\documentclass{article}

\usepackage[english]{babel}


\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algorithmicx}

\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{example}{Example}

\newcommand{\Thm}[3]{\begin{theorem}[#1]\label{#2}#3\end{theorem}}
\newcommand{\Ex}[3]{\begin{example}[#1]\label{#2}#3\end{example}}
\newcommand{\Def}[3]{\begin{definition}[#1]\label{#2}#3\end{definition}}
\newcommand{\Lem}[3]{\begin{lemma}[#1]\label{#2}#3\end{lemma}}
\newcommand{\Cor}[3]{\begin{corollary}[#1]\label{#2}#3\end{corollary}}
\newcommand{\Prop}[3]{\begin{proposition}[#1]\label{#2}#3\end{proposition}}
\newcommand{\Rem}[3]{\begin{remark}[#1]\label{#2}#3\end{remark}}
\newcommand{\Proof}[1]{\begin{proof}#1\end{proof}}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\X}{\mathcal{X}}


\begin{document}
	
\title{Computer Algebra}
\author{Guillaume}

\maketitle
\tableofcontents

...(course 1)

Karatsuba multiplication : $O(n^{1.59\dots})$

$(a_1X+a_0)(b_1X+b_0) = a_1b_1X^2 + a_0b_0 + ((a_1+a_0)(b_1+b_0) - a_1b_1 - a_0b_0)X$

\begin{itemize}
	\item $a_0$ can be seen as A(0)
	\item $a_0 + a_1$ as A(1)
	\item $a_1$ as A($\infty$)
\end{itemize}
So, $A(X)\times B(X)$ is characterized by its value at 3 points: 0,1,$\infty$.

We can find plenty of such formula with 0,1,-1.

If $A.B = c_2X^2+c_1X+c_0$,

$c_0 = a_0b_0$, $c_1=\frac{1}{2}(A.B(1) - A.B(-1))$, $c_2 = \frac{1}{2}(A.B(1)+A.B(-1)) - A.B(0)$

\subsection{Tom-Cook method}
Let A(X),B(X) have degree r.
In order to compute A(X).B(X) using evaluation interpolation, I need to do :
\begin{itemize}
	\item 2r-1 evaluations
	\item 2r-1 pointwise products
	\item interpolation in degree 2r-1
\end{itemize}

As in Karatsuba's method, we use this idea on polynomials of degree n$>>$r, say P,Q:

Put:

$\tilde{P}(X,T)= \sum_{i=0}^{r} P_i(X)T^i$\\
$\tilde{Q}(X,T) = \sum_{i=0}^{r} Q_i(X)T^i$

with $\tilde{P}(X,X^{\frac{n}{r}}) = P(X)$\\
$\tilde{Q}(X,X^{\frac{n}{r}}) = Q(X)$

$\deg_T \tilde{P}(X,T) \leq r$
$\deg_T \tilde{Q}(X,T) \leq r$
\subparagraph{}
Cost of evaluation-interpolation ?
\begin{itemize}
	\item evaluation : O(r) times O(r) additions of poly's of degree $\frac{n}{r}$ = O(rn)
	\item pointwise products : 2r+1 multiplies of polynomials of degree $\frac{n}{r}$
	\item interpolation : O(n.poly(r))
\end{itemize}

Overall, if TC$_r$(n) stands for the cost of multiplication, and assuming we use recursion for the pointwise products, we get :
$\text{TC}_r(n) = (2r+1)\text{TC}_r(\frac{n}{r})+O(n.\text{poly}(r))$

(For r=1 we get Karatsuba)

For fixed r, we get

$\text{TC}_r(n) = O(n^{\frac{\log(2r+1)}{\log r}})$

($n=r^j$, $\text{TC}_r=(2r+1)\text{TC}_r(r^{j-1})+\dots$

TC$_c(n) = (2r+1)^{\log_{r}n}$

Maybe r should be replaced by r+1.

Tempting to let r$\rightarrow\infty$ to get O(n$^{1+\epsilon}$)

It can be done but requires some care so that evaluation \& interpolation step remain negligible - one needs to keep r = n$^{o(1)}$, e.g. r = exp($\sqrt{\log n}$)

In practice: used for r = 3(,4), then FFT becomes better.

\subsection{FFT}
Key idea: Using a suitable set of points (namely roots of 1) yieldss a very fast evalution/interpolation

In the sequel, we'll assume that
$\deg P < r = 2^k$ (so k = $\lceil \log_2(\deg P + 1)\rceil$ and the base field contains all r-th roots of unity. We let $\omega$ be a primitive such roots (so that {r-th roots of 1} = {$\omega^j$, $0\leq j \leq r-1$})

Identification :
$P(X) = \sum_{j=0}^{r-1} p_j X^j \longleftrightarrow P = (p_0,\dots,p_{r-1})$

Define the Discrete Fourrier Transform fo size r to be

$\text{DFT}_{w,r}(P) = (P(\omega^0),\dots,P(\omega^{r-1})$

\begin{lemma}
	DFT$_{w,r}(P) \odot \text{DFT}_{w,r}(Q) = \text{DFT}_{w,r}(PQ)$
	
	where $\odot$ is coordinatewise product of vectors
\end{lemma}

\begin{proof}
	Follows from the definition
\end{proof}

\begin{lemma}
	DFT$_{w^{-1},r}(\text{DFT}_{w,r}(P)) = rP$
	
	So DFT$^{-1}_{w,r} = \frac{1}{r}\text{DFT}_{w^{-1},r}$
\end{lemma}
\begin{proof}
	Let $Q(X) = \text{DFT}_{w,r}(P)$ seen as a polynomial $= \sum_{i=0}^{r-1} P(\omega^i)X^i$
	\begin{equation*}
		=\sum_{i=0}^{r-1}\sum_{j=0}^{r-1} p_j \omega^{ij} X^i	
	\end{equation*}
	\begin{align*}
	\text{DFT}_{w^{-1},r}(Q)\mid_l & =  Q(\omega^{-l})\\
	& = \sum_{i=0}^{r-1}\sum_{j=0}^{r-1} p_j \omega^{ij} \omega^{-il}\\
	& = \sum_{j=0}^{r-1}p_j \sum_{i=0}^{r-1}\omega^{i(j-l)}
	\end{align*}
\end{proof}
\begin{claim}
	If $\alpha^r = 1$, then $\sum_{i = 0}^{r - 1}\alpha^i = $ r if $\alpha = 1$, and 0 otherwise
\end{claim}
Indeed, if $\alpha = 1$, it is a clear fact; otherwise let $\sigma$ stand for the sum :

Then 
\begin{align*}
\alpha S & = \sum_{i=0}^{r-1} \alpha^{i+1}\\
& = \sum_{i=1}^{r} \alpha^{i}\\
& = \sum_{i=1}^{r-1} \alpha^{i} + \alpha^r = S
\end{align*}
Thus $(\alpha-1)S = 0$ and $\alpha \neq 1 \implies S=0$

Hence $\sum_{i=0}^{r-1} \omega^{i(j-l)} = $ r if $\omega^{j-l} = 1 \Leftrightarrow j \equiv l$ (mod r), and 0 otherwise

as $0\leq j,l \leq r$

$j \equiv l $(mod r) $\Leftrightarrow j = l$
(End of proof)

Bottomline : if we have a fast algorithm for DFT, we have a fast algorithm for DFT$^{-1}$

\begin{theorem}
	DFT$_{w,r}$(P) can be computed in time O(r log r) operations in the base field
\end{theorem}
\begin{proof}
	Split P as $P(X) = P_{\text{even}}(X^2) + X P_{\text{odd}}(X^2)$
	
	Then
	\begin{align*}
	\text{DFT}_{\omega,r}(P)\mid_l & =  P_{\text{even}}(\omega^{2l}) + \omega{l} P_{\text{odd}}(\omega^{2l})\\
	& = (\text{DFT}_{\omega^2,\frac{r}{2}}(P_{\text{even}}) + \text{DFT}_{\omega^2,\frac{r}{2}}(P_{\text{odd}}) \odot \text{DFT}_{\omega^2,\frac{r}{2}}(X))\\
	& \parallel  (\text{DFT}_{\omega^2,\frac{r}{2}}(P_{\text{even}}) - \text{DFT}_{\omega^2,\frac{r}{2}}(P_{\text{odd}}) \odot \text{DFT}_{\omega^2,\frac{r}{2}}(X))
	\end{align*}
	(The concatenation is possible because $\omega^{r+l} = \omega^l$ and the $-$ comes from $\omega^{\frac{r}{2}} = -1$)
\end{proof}

Algorithm : FFT($\omega$,r,P)\\
If r == 1, return (P(1))\\
D$_1$ = FFT($\omega^2,\frac{r}{2},P_{\text{even}}$)\\
D$_2$ = FFT($\omega^2,\frac{r}{2},P_{\text{odd}}$)\\
For i from 0 to r-1 do:


	D$_2\mid_i$ = D$_2\mid_i \times \omega^i$\\
Return $D_1 + D_2 \parallel D_1 - D_2$

\subparagraph{}
Say F(r) is the cost of the algorithm on $\omega$,r,P
Then $F(r) = 2F(\frac{r}{2}) + O(r) + O(r)$ (first is the loop, second if the sum/difference)

So F(r) = O(r log r).

\subparagraph{Multiplication algorithm}
Let P,Q have degree < $\frac{r}{2} = 2^{k-1}$
\begin{enumerate}
	\item Compute $D_P = \text{DFT}_{\omega,r}(P)$, $D_Q = \text{DFT}_{\omega,r}(Q)$
	\item $D = D_P \odot D_Q$
	\item Return $\text{DFT}_{\omega{-1},r}(D)$
\end{enumerate}
Cost O(r log r)

Correctness : follows from the 2 lemmas + deg(PQ) < r

Cost : 3 DFTs of size r + O(r) for the componentwise products

\subparagraph{Application}
\begin{itemize}
	\item Polynomial product over $\mathbb{K}$[X] have to choose a suitable base field.
	Most common :
	$\mathbb{K} \subseteq \mathbb{C}$ or $\subseteq \mathbb{Z}/p\mathbb{Z}$ where $p \equiv 1$ mod r
	\item Integer multiplication much more difficult
	
	Schönehage-Strassen achieve O(n log n log log n)
	Fürer (2000's) achieved O(n log n $2 ^{\log_* n})$
\end{itemize}
\subparagraph{Open question}
	Is it possible to multiply 2 polynomials faster than n log n ?
\subparagraph{Summary}
Summary\\
Poly | int\\
$n^2$ |  " (naive)\\
$n^{1.59}$ | " (Karatsuba)\\
$n^{\frac{\log(2r+1)}{log(r+1)}}$ | " (TC$_r$)\\
$n \log n$ | $ \log n \log \log n$ (FFT)\\

\subparagraph{}
In the sequel we'll assume M(n) increasing and superlinear ($M(\sum n_i) \geq \sum M(n_i)$)

\section{Interlude: Hensel's lemma/Newton method}
Newton method :
f(X) = 0, f sufficiently regular\\
Tangent at $x_i$ : $y - f(x_i) = f'(x_i)(x-x_i)$
most of the time it works fine, and if $x_0$ is a "good guess"
$x_{i+1} - \alpha \approx (x_i - \alpha)^2$ meaning that the number of correct digits doubles at each step : $|x_n - \alpha| \approx c^{2^n}, c < 1$

\Thm{Hensel Lemma}{}{Let $F \in \K[X,Y]$ (We want to solve $F(X,P(X))=0 \mod X^N$ for some $P \in \K[X]$)\\
	Assume that $F(0,s_0) = 0$ and $\frac{\partial F}{\partial Y}(0,s_0)\neq 0$\\
	Then $\forall n, \exists!S_n$ of degree $ < 2^n$ s.t\\
	$S_n(0)=S_0$ and $F(X,S_n(X))=0 \mod X^{2^n}$\\defined by $S_0 = s_0, S_{n+1} = S_n - \frac{F(X,S_n(X))}{\frac{\partial F}{\partial Y}(X,S_n(X))}$}
\Proof{By induction\\Assune $S_n$ is known + unique.
If for some $S_{n+1}$, $F(X,S_{n+1}(X)) = O(\mod X^{2^{n+1}})$\\
Then a fortiori, $F(X,S_n(X) \mod X^{2^{n+1}}) = O(\mod X^{2^n})$\\
By uniqueness of $S_n$, $S_{n+1} \equiv S_n \mod X^{2^{n}}$\\
Hence, $S_{n+1} = S_n + X^{2^{n}} T_n$ for some $T_n$\\
\begin{align*}
F(X,S_{n+1}(X)) & = F(X,S_n(X)) + (S_{n+1}(X)-S_n(X))\cdot\frac{\partial F}{\partial Y}(X,S_n(X))\\ & + (S_{n+1}(X) - S_n(X)^2(\equiv O(\mod X^{2^{n+1}}) \cdot[\star] (\in \K[X])
\end{align*}
Modulo $X^{2^{n+1}}$, we get\\
$(S_{n+1} - S_n)\cdot\frac{\partial F}{\partial Y}(X,S_n(X)) = -F(X,S_n(X)) 
\mod X^{2^{n+1}}$\\
Now $\frac{\partial F}{\partial Y}(X,S_n(X))|_0 = \frac{\partial F}{\partial Y}(0,s_0) \neq 0$ by assumption;\\
So $\frac{\partial F}{\partial Y}(X,S_n(X))$ coprime with X, hence invertible mod $X^{2^{n+1}}$\\
so $S_{n+1} = S_n - \frac{F(X,S_n(X))}{\frac{\partial F}{\partial Y}(X,S_n(X))} \mod X^{2^{n+1}}$}

\Rem{}{}{\begin{itemize}
		\item This is the so-called quadratic version of Hensel lemma
		\item There is a linear version, replacing $S_n$ by $T_n$ and $X^{2^{n}}$ by $X^n$ everywhere - works exactly the same
		\item In practice Hensel's lemma is often used recursively
		\item Recursive call $F(X,\tilde{S}(X)) = 0 \mod X^{\lceil N/2 \rceil}$ + do $S = \tilde{S} - \frac{F(X,\tilde{S}(X))}{\frac{\partial F}{\partial Y}(X,\tilde{S}(X))} \mod X^{N}$
	\end{itemize}}
Complexity "meta-analysis" with F having O(1) degree\\
$H(N) = H(N/2) + O(M(N))\text{(F and derivate)} + O(D(N))\text{(division)}$\\
($O(D(N)) = O(M(N))$)\\
So \begin{align*}
H(N) & = H(N/2) + O(M(N))\\
& = O(M(N)) + O(M(N/2)) + O(M(N/4)) + ... + O(M(N/2^k)) (2^k \approx N)\\
& \leq O(M(N+N/2+N/4+\dots+N/2^k))\\
& \leq O(M(2N)) = O(M(N))
\end{align*}

Inversion : $P^{-1} \mod X^{2^n}$ ?\\
\begin{align*}
S_{n+1} & = S_n - \frac{P(X)S_n - 1}{P(X)} \mod X^{2^n}\\
& = 1/P(X) \mod X^{2^n}\\
& = S_n - \frac{1/S_n(X) - P(X)}{-1/S_n(X)} \mod X^{2^{n+1}}\\
& = S_n + S_n(1 - S_n(X)P(X)) \mod X^{2^{n+1}}\\
\end{align*}
\begin{claim}
	$PS_n \equiv 1 \mod X^{2^n}$
\end{claim}
Recurvsive version. Say $\tilde{S}$ s.t $P\tilde{S} = 1 \mod X^{\lceil N/2 \rceil}$\\
then $S = \tilde{S} + \tilde{S}(1 - P\tilde{S}) \mod X^{N}$ is s.t $PS \equiv 1 \mod X^N$\\$I(N) = I(N/2) + 2M(N/2) + O(N) \rightarrow I(N) = O(M(N))$\\
Hence division mod $X^N$ costs $I(N)+M(N) = O(M(N))$\\

Normal,Euclidian division\\
Let $A,B \in \K[X]$ + say we look for $Q,R$ with $A = BQ + R, \deg R < \deg B$\\
If P polynomial, define $P^{\#}(X):=X^{\deg P}\cdot P(1/X)$

\begin{claim}
	$Q^\# = \frac{A^\#}{B^\#} \mod X^{\deg Q + 1}$
\end{claim}
\Proof{\begin{align*}
	A(\frac{1}{X}) & = B(\frac{1}{X})Q(\frac{1}{X}) + R(\frac{1}{X})\\
	X^{\deg A} A(\frac{1}{X}) & = (X^{\deg B} B(\frac{1}{X}))\cdot(X^{\deg A - \deg B}Q(\frac{1}{X})) + X^{\deg A} R(\frac{1}{X})\\
	A^\# & = B^\# Q^\# + X^{\deg A - \deg R}R^\#\\
	& \deg A \deg R > \deg A - \deg B = \deg Q\\
	& \text{so } X^{\deg A - \deg R} = O(\mod X^{\deg Q + 1})\\
	\end{align*}
	+ note by def that $B^\#(0) \neq 0$}

Overall, Euclidian division(A,B):\\
Compute $T = \frac{A^\#}{B^\#} \mod X^{\deg A - \deg B + 1}$\\
Return $T^\#, R = A-BT^\#$\\

Cost if $\deg A = 2N$, $\deg B = N \rightarrow O(M(N))$\\

Comment\\
There is another version of fast division: By considering $A_{high}$ and $A_{low}$ with cost $\tilde{D}(n) = 2\tilde{D}(n/2) + O(M(N))\\
\tilde{D}(n) = O(M(n))$ if $M(n)\approx n^\alpha, \alpha > 1\\
\tilde{D}(n) = O(M(n)\log n) if M(n) \approx n\log^\beta n$\\

In practice,\begin{itemize}
	\item in the quasi-linear range, Hensel based is better
	\item in the $n^\alpha$ range, the constant in "recursive division" makes it slightly better.
\end{itemize}

\subsection{Square root}
$F(X,Y) = Y^2 - P(X)$\\
If $\tilde{S}$ is s.t $\tilde{S}(X)^2 \equiv P \mod X^{\lceil N/2 \rceil}$, then \begin{align*}
S & = \tilde{S} - \frac{\tilde{S}^2 - P(X)}{2 \tilde{S}} \mod X^{2^{n+1}} (X_{n+1} = 1/2(X_n + a/X_n))\\
& = 1/2(\tilde{S} +  P(X)/\tilde{S}) \mod X^{2^{n+1}}
\end{align*}

Cost $S(N) = S(\lceil N/2 \rceil) + O(D(N)) + O(N)$\\
$\implies S(N) = O(M(N))$

\Rem{}{}{Slightly faster in practice to compute $\bar{S}\\
	\bar{S}^2 = 1/P \mod X^{2^n}$ using $F(X,Y) = 1/Y^2 - \bar{S}(X)$\\
	return $S = P\bar{S}$ (as "$\bar{S} = 1/\sqrt{P}", "S=\sqrt{P}$)\\}

Base change:\\
$P(X) = \sum_{i=0}^{N-1} p_iX^i$\\
B(X) of degree deg B\\
Want to find $b_0,b_1,...,b_r \in \K[X], \deg b_i < \deg B$ s.t $P(X) = \sum_i b_i(X)B(X)^i$\\
Naive version:
$i \gets 0$\\
While deg $P \neq 0$ do\\
$b_i \gets P \mod B\\
P \gets \frac{P-b_i}{B}\\
i \gets i+1$\\
End\\

Complexity : $\frac{deg P}{deg B}$ steps $(\deg P)(\deg B) = (\deg P)^2$\\

Right version:\\
$P_l = P \mod B^{1/2*\lfloor\frac{\deg P}{\deg B}\rfloor}\\
P_l = \frac{P - B_l}{B^{1/2\lfloor\frac{\deg P}{\deg Q}\rfloor}}\\$
Return basechange($P_h,B)||$basechange($P_l,B$)

BC(N) = 2BC(N/2) + D(N) + 2(N)\\
Cost of $B^l$ = C(l)
By fast exponentiation, $C(l) \leq O(M(2l\deg B)) = O(M(l\deg B))$

\section{GCD, extended GCD, Chinese remainder theorem}
Over a ring R = $\Z$ or $\K[X]$
\subsection{Euclidean algorithm}
Input: a,b\\
Output a gcd of a,b\\
(b==0)?a:gcd(b,Remainder(a,b))

\Proof{Loop invariant: gcd(a,b) does not change}

\subsection{Analysis}
\subsubsection{Number of steps}
$\| x \| := \log_2(1+|x|),x\in\Z\\
 \deg x + 1, x\in\K[X]$

\Prop{}{}{The number of steps in Euclid's algorithm is O(1 + min($\| a \|,\| b \|) - \| gcd(a,b)\|)$}
\Proof{Common step to R=$\Z \& \K[X]$\\
	Step 1: divide a by b, then we are reduced to a gcd(a',b') with $\| a' \|,\| b' \| \leq$ min($\| a' \|,\| b' \|)$\\
	R = $\K[X]$\\
	At every step, deg r$_i$ decreases by at least 1\\
	Goes from min($\| a \|,\| b \|$) to $\| gcd(a,b)\|$\\
	R = $\Z$\\
	\begin{claim}[Lamé]
		After the beginning and as long as a,b$\neq$ 0, in two steps max(a,b) decreases at least by a factor of 2.
	\end{claim}
	\Proof{$r_k = q_k*r_{k+1}+r_{k+2}\\
		r_{k+1} = q_{k+1}*r{k+2}+r_{k+3}$\\
		If $r_{k+1} \leq r_k/2$\\
		then $r_k$ = max($r_k,r_{k+1}) \leq 1/2$ max($r_{k+2},r_{k+1})$ = $r_{k+1}$\\
		Same for $r_{k+1} \geq r_k/2$\\
		$r_{k+2} \leq r_k - r_{k+1}$ as q$_k \geq 1$\\}
	Goes from min($\| a \|,\| b \|$) to $\| gcd(a,b)\|$ 2 steps by 2 steps
}

\Rem{}{}{This bound is sharp for consecutive Fibonacci number}

\subsubsection{Bitcomplexity}
\Thm{?}{}{Let A = max($\| a \|,\| b \|$), B = min($\| a \|,\| b \|$), D = $\| gcd(a,b)\|$\\
	The overall complexity of Euclidean algorithm is O((B+1)(A-D+1))}

\Rem{}{}{This is better than what we might have expected, which would be O(B-D) steps of cost O(AB)}
The point is that \begin{itemize}
	\item either a division is costly, but then a,b are reduced a lot (so fewer steps)
	\item or we do many steps but the divisions are cheap
\end{itemize}

\Proof{
	$r_{i-1} = q_ir_i + r_{i+1}$\\
	Costs : $O(\|q_i \| \| ri \|) = O((\|r_{i-1}\| - \|r_i \| + 1)\|r_i\|)$ (+1 comes from the difference between $\|r_i\|$ and deg($r_1$))\\
	
	Total cost:
	\begin{align*}
		&O(\sum\limits_{i = 0}^{\#\text{steps}} (\|r_{i-1} \| - \|r_i\| + 1)\|r_i\|) & \|r_i\| \leq \|r_1\| = B\\
		=& O(B(\sum\limits_{i = 0}^{\#\text{steps}} \|r_{i-1} \| - \|r_i\| + 1))\\
		=& O(B(\#\text{steps}+1 + \|r_0\| - \|r_{\#\text{steps}}\|)) &\#\text{steps} \leq B-D, \|r_0\| = A, \|r_{\#\text{steps}}\| = D\\
		=& O(B(A-2D+B+1))
	\end{align*}
	
	proving that this is O((B+1)(A-D + 1)) is left as an exercise (the B+1 comes from neglecting possible an inversion at the first step, otherwise : use $B-D \leq A-D$ so $A-2D+B = O(A-D)$}

\Rem{}{}{One can prove that, for random inputs, a,b, Pr(given $q_i = k) \approx 1/k^2$\\
	As a consequence, it might sometimes be better to use successive subtractions than division in Euclid}

\subsection{Binary algorithm}
Give it for $\Z$\\
Easy to turni it into a $\K[X]$ algo by letting X paly the role of 2.

Idea: if x = $2^{v(x)}x'$, y = $2^{v(y)}y'$ with x',y', odd\\
$\gcd(x,y) = 2^{min(v(x),v(y))}\cdot\gcd(x',y')$\\
Say $x'\geq y'\geq 0$,\\
gcd(x',y') = gcd(y',x'-y') = gcd(y',$\frac{x'-y'}{2^{v(x'-y')}}$)\\
Continue until y' = 0, return $x'\cdot 2^{min(v(x),v(y)}$\\

Analysis: \begin{itemize}
	\item \# steps: at every step, xy decreases by $\leq 2$, so, \# steps $\leq \|xy\| = O(max(\|x\|,\|y\|))$
	\item One step is a subtraction with linear cost:\begin{itemize}
		\item over $\K[X]$ cost O(min($\|x\|,\|y\|))$ by doing in place
		\item over $\Z$ cost O(max($\|x\|,\|y\|))$
	\end{itemize}
	Similar quadratic complexity
	
\subsection{Extended Euclidean algorithm}
Purpose: Given a,b find d = gcd(a,b), u,v with au+bv = d\\
Caution: Exists over $\Z,\K[X]$ (over a PiD = principal ideal domain (fr = anneau principal))\\

$r_{i-1} = q_ir_i + r_{i+1}$ can be seen as\\
$\begin{pmatrix}
	r_i\\r_{i+1}
\end{pmatrix}
=
\begin{pmatrix}
0 & 1\\ 1 & -q_i
\end{pmatrix}
\begin{pmatrix}
r_{i-1} \\ r_i
\end{pmatrix}$\\

So: $\begin{pmatrix}
\gcd(a,b)\\0
\end{pmatrix}
=
\begin{pmatrix}
u_k & v_k\\ u_{k+1} & v_{k+1}
\end{pmatrix}
\begin{pmatrix}
a \\ b
\end{pmatrix}$\\

$u_ka+v_kb = \gcd(a,b)$\\

Compute the product on the fly from right to left.\\
\begin{align*}
M_0 = & \begin{pmatrix}
0 & 1\\ 1 & 0
\end{pmatrix}\\
M_j = & \begin{pmatrix}
u_j & v_j\\ u_{j+1} & v_{j+1}
\end{pmatrix}\\
\text{with } M_{j+1} = & \begin{pmatrix}
0 & 1\\ 1 & -q_{j+1}
\end{pmatrix}
M_j
\end{align*}

which gaves $\left\{\begin{array}{r c l r}
u_{i+1} & = & u_{i-1} - q_iu_i & (\star)\\
v_{i+1} & = & v_{i-1} - q_iv_i
\end{array}\right .$
\end{itemize}

\begin{claim}
	$\forall i, u_ia+v_ib=r_i$
\end{claim}
\Rem{}{}{$u_0,v_0$ small $\rightsquigarrow$ $u_k,v_k$ large\\
	$r_0,r_1$ large $\rightsquigarrow$ $r_k$ small}

More formally: I claim $\deg(u_ia) = \deg(v_ib)$ for $i\geq 2$ as $\left[\begin{array}{r c c cl}
\deg(r_i) & < & \deg(a) & \leq & \deg(u_ia)\\
\deg(r_i) & < & \deg(a) & \leq & \deg(v_ib)
\end{array}\right.$\\
So $\deg u_i + \deg a = \deg v_i + \deg b$\\\\
I claim that $\deg u_{i+1} = \deg u_i + \deg q_i$ (follows from ($\star$) and $\deg u_i \nearrow$ for $i \geq 2$)\\
But $q_i =$ quotient$(r_{i-1},r_i)$, so $\deg q_i = \deg(r_{i-1}) - \deg(r_i)$\\
So \begin{align*}
	\deg u_{i+1} = \deg u_i + & \deg r_{i-1} - \deg r_i\\
	\deg u_{i+1} + \deg r_i = & \deg u_i + \deg r_{i-1}\\
	= & \text{ constant not depending on i}
\end{align*}
 
\begin{claim}
	$u_iv_{i+1} - u_{i+1} v_i = (-1)^i$
\end{claim}
This is because $\det(M_i) = \begin{pmatrix}
0 & 1 \\ 1 & -q_i
\end{pmatrix} \det(M_{i-1})$ and $\det M_0 = 1$

\subsection{Quasilinear (a.k.a. "Fast") GCD}
\Lem{}{}{Say u,v,u',v' $\in \Z^4 \setminus \{0,0,0,0\}$\\such that $\left\{\begin{array}{r c l}
	\alpha & = & ua + vb\\
	\beta & = & u'a + v'b
	\end{array}\right.$, $|uv'-u'v| = 1$\\
	Then gcd$(\alpha,\beta) = \gcd(a,b)$}
\Proof{Obviously, $\begin{array}{c}
		\gcd(a,b)\mid\alpha\\
		\gcd(a,b)\mid\beta
	\end{array} \implies \gcd(a,b)\mid\gcd(\alpha,\beta)$\\
	
	$\begin{pmatrix}
		\alpha\\\beta
	\end{pmatrix}
	= \begin{pmatrix}
		u&v\\u'&v'
	\end{pmatrix}
	\begin{pmatrix}
	a\\b
	\end{pmatrix}$\\
	
	But$\begin{pmatrix}
	u&v\\u'&v'
	\end{pmatrix}
	= \frac{1}{u'v-uv'}
	\begin{pmatrix}
	v'&-v\\-u'&u
	\end{pmatrix}
	= \pm \begin{pmatrix}
	v'&-v\\-u'&u
	\end{pmatrix}$\\
	
	So 	$\begin{pmatrix}
	a\\b
	\end{pmatrix}
	= \pm \begin{pmatrix}
	v'&-v\\-u'&u
	\end{pmatrix}
	\begin{pmatrix}
	\alpha\\\beta
	\end{pmatrix}$ which by the sale argument as before, implies $gcd(\alpha,\beta)\mid\gcd(a,b)$
		}
		
Let's try to do a fast algorithm for GCD, EEA (Extended Euclidean Algorithm)\\
We're going to mimic recursively division.\\
deg a, deg b = 2n\\
$a = a_{hi}X^n + a_{lo}\\
b = b_{hi}X^n + b_{lo}$\\
Recursive call on $a_{hi},b_{hi}\hookrightarrow \gcd(a_{hi},b_{hi})$\\

Let's assume everything is "generic", then gcd($a_{hi},b_{hi}) \approx 1$\\
So deg x, deg y, deg x', deg y' $\approx$ n\\

Now, gcd(a,b) = gcd(ax+by,ax'+by') (Lemma + Claim)\\\\
ax'+by' = $(a_{hi}x'+b_{hi}y')X^n + a_{lo}x' + b_{lo}y'$\\\\
FAILURE: started with a,b of degree 2n, went to ax+by, ax'+by' of degree 2n\\

Diagnosis: Need a better balance between the two terms\\

Define HalfGCD(a,b) $\rightarrow \begin{pmatrix}
x & y\\x' & y'
\end{pmatrix}$ s.t deg x,y,x',y' $\approx$ n (instead of 2n), deg ax+by, ax'+by' $\approx$ n (insteal of $\Theta$\textbf{???})\\

Do HalfGCD recursively\\
Let $\begin{pmatrix}
x_h&y_h\\x'_h&y'_h
\end{pmatrix} = $HalfGCD$(a_{hi},b_{hi})$\\
Then deg $x_h,y_h,x'_h,y'_h \approx n/2, \deg a_{hi}x_h+b_{hi}y_h \approx n/2, \deg a_{lo} x_h + b_{lo} y_h \approx 3n/2$, so deg  $ax_n + by_n \approx 3n/2$\\\\
$\begin{pmatrix}
x_l&y_l\\x'_l&y'_l
\end{pmatrix} \leftarrow \text{HalfGCD}(a'_{hi},b'_{hi})$

\begin{claim}
	$\begin{pmatrix}
	u&v\\u'&v'
	\end{pmatrix}
	=
	\begin{pmatrix}
	x_l&y_l\\x_l'&y'_l
	\end{pmatrix}
	\begin{pmatrix}
	x_h&y_h\\x_h'&y'_h
	\end{pmatrix}$
\end{claim}

Then $\left\{\begin{array}{r c l}
\deg au+bv & \approx & n\\
\deg au'+bv' & \approx & n\\
\deg u,u',v,v' & \approx & n
\end{array}\right .$\\
So HalfGCD(a,b) returns $\begin{pmatrix}
u&v\\u'&v'
\end{pmatrix}$\\\\

To sum up, HalfGCD(a,b), deg a,b = 2n reduces to 2 calls HalfGCD($a_{hi},b_{hi})$, HalfGCD($a_{lo},b_{lo})$ of degree n + a bunch of degree n multiplications.\\
So HG(2n) = 2HG(n) + O(M(n)) $ \implies$ HG(n) O(M(n)) if M(n) = n$^\alpha$, O(M(n)log n) if M(n) = n(log n)$^\beta$

GCD(a,b) (d° 2n):\\
HalfGCD(a,b) $\rightarrow \begin{pmatrix}
u&v\\u'&v'
\end{pmatrix}$\\
such that d°(au+bv) $\approx$ d°(au'+bv') $\approx$ n, gcd(au+bv,au'+bv') = gcd(a,b)\\\\
Recursive GCD(au+bv,au'+bv') (d° n)\\

Cost: G(2n) = HG(2n) + G(n) + O(M(n))\\
G(2n) = O(HG(2n))

\Rem{}{}{over $\Z[X]$ or $\K[X,Y]$}

\Lem{Gauss Lemma}{}{Let R = $\Z[X]$ or $\K[X,Y]$ (a UFD (anneau factoriel)).\\
	Define P = $\sum\limits_{i=0}^n p_iX^i$ (where $p_i \in \K[X]$ if $R = \K[X,Y])$\\
	Q =  $\sum\limits_{i} q_iX^i$\\
	cout(P) = gcd(($p_i)_{0\leq i\leq n}$)\\
	Then cout(gcd(P,Q)) = gcd(cout(P),cout(Q))}

\Ex{}{}{P = 2+4X+6X$^2$+2X$^3$, cout(P) = 2}

From our point of view:\\
Given P,Q $\in \Z[X]$ or $\K[X,Y]$\begin{itemize}
	\item Compute $R_0 = \gcd(P,Q)$ over $\Q[X]$ or $\K(Y)[X]$
	\item clear the denomiators to get $R_1 \in \Z[X]$ or $\K[X,Y]$ with content 1 (multiply by the lcm of all denominators)\\
	Note that lcm(a,b) = a.b/gcd(a,b)
	\item Return $R_1\times\gcd($cout$(P),$cout$(Q)$
\end{itemize}

\subsubsection{Poor man's elimination}
Say you have 2 curves $\left\{\begin{array}{r c l}
P(X,Y) & = & 0\\
Q(X,Y) & = & 0
\end{array}\right.$

Do EEA on P(X,Y), Q(X,Y) over $\K(Y)[X]$:\\
U(X,Y)P(X,Y)+V(X,Y)Q(X,Y) = $\left\{\begin{array}{l}
1\\
\gcd(P(X,Y),Q(X,Y)
\end{array}\right.$ where U,V $\in \K(Y)[X]$\\
Say the gcd is 1 then, clearing the denominators, we get $\bar{U},\bar{V}\in\K[X,Y],W\in\K[Y]$, s.t\\
$\bar{U}(X,Y)P(X,Y)+\bar{V}(X,Y)Q(X,Y) = W(Y)$\\

Say ($x_0,y_0$) is a solution of the system above, then $W(y_0) = 0$

\subsection{Polynomial interpolation \& evaluation}
\subsubsection{Evaluation}
\underline{One point evaluation}\\

The classical Horner scheme\\
Input: U = $\sum_{i=0}^{d_u} u_iX^i$, a$\in\K$\\
Output: U(a) = $(\dots((u_{d_u}a+u_{d_u-1})a + u_{d_u-2})a+\dots)+u_0$\\\\
$r \leftarrow u_{d_u}$\\
for i from $d_u-1$ downto 0, do $r \leftarrow a\cdot r+u_i$\\
return r\\\\

Cost: deg U multiplies, deg U adds\\

\underline{Interlude-Product tree}\\
Q: Given $x_1,\dots,x_n \in\K$, how do we compute $\prod\limits_{i=1}^{n}(X-x_i)$ and at what cost ?\\\\

1st idea: $[\dots[[(X-x_1)(X-x_2)](X-x_3)]\dots]$\\
We use: $\prod\limits_{i=1}^{n}(X-x_i) = \prod\limits_{i=1}^{n-1}(X-x_i) \times (X-x_n)$\\
So $C(n) = C(n-1) + O(n) = O(n^2)$\\

Better: \begin{itemize}
	\item Compute $\prod\limits_{i=1}^{\lfloor n/2 \rfloor}(X-x_i) = Q_0(X)$
	\item Compute $\prod\limits_{i=\lfloor n/2\rfloor+1}^{n}(X-x_i) = Q_1(X)$
	\item Return $Q_0(X)Q_1(X)$
\end{itemize}
Cost: $\bar{C}(n) = 2 \bar{C}(n/2) + M(n/2) = O(M(n)\log(n))$\\
or even $O(M(n))$ if M(n) = $n^\alpha, \alpha > 1$

\underline{Multiple point evaluation}
Say P has degree n-1 and we want $P(x_1),\dots,P(x_n)$\\

Key remarks: $P(X) \equiv P(x_i) mod (X-x_i)$\\
$X - x_i | P(X) - P(x_i)$ as $x_i$ is a root of $P(X) - P(x_i)$\\

Formally, if n == 1, return P (which is a constant)\\
Compute $P_0 = P \mod Q_0 = \prod\limits_{i=1}^{\lfloor n/2 \rfloor}(X-x_i)$\\
$P_1 = P \mod Q_1 = \prod\limits_{i=\lfloor n/2\rfloor+1}^{n}(X-x_i)$\\

Recursive evalutation of $P_0$ at $x_1,\dots,x_{\lfloor n/2\rfloor}$ and $P_1$ at $x_{\lfloor n/2\rfloor+1},\dots,x_n$\\

Cost: \begin{itemize}
	\item 1 product tree O(M(n)log n)
	\item 2 divisions of size n by n/2 $\implies$ 2D(n/2)
	\item Z recursive calls in size n/2
\end{itemize}
Overall, evaluation costs O(M(n)log n)\\

What if deg P $\neq$ n-1 ?\\
if deg P $>$ n-1\\
Reduce to deg P = n-1 by doing $\tilde{P} = P mod Q$\\
cost is O(M(n)log n) + division d by n, which is eother O(M(d)) (fast) or O(dn) (naive)\\

d $<$ n: n/d times evaluations at d points $\rightarrow O(\frac{n}{d}M(d)\log d) \approx O(n \log^2d)$ with FFT (instead of O(n log$^2$n))

\Rem{}{}{All those complexisties can be reduced to O(M(n)) if $x_i = \alpha^i$ for some $\alpha\in\K$}

\Rem{}{}{Arithmetic analogue, take N$\in\Z$ large, $p_1,\dots,p_r$ primes, and we want $N \mod p_1,\dots,N \mod p_r$}

\subsubsection{Interpolation}
\underline{Lagrange formula}\\
$L_i(X)$ polynomial $L(x_j) = \delta_{i,j}$\\

General problem:\\
Given $(x_i,y_i)_{1\leq i\leq n}$, find P(X) s.t $P(x_i) = y_i \forall i,\deg P \leq n-1$\\

$P = \sum_{i = 1}^{n}y_i L_i(X)$\\
Cost: each $L_i: O(n^2) or O(M(n)\log n)$, combinaison in $O(n^2)$, overall $O(n^3)$ or $O(n(M(n)\log n))$\\

Better: compute Q(X) = $\prod\limits_{i=1}^{n}(X-x_i)$\\
$L_i(X) = \frac{Q(X)}{(X-x_i)Q'(x_i)}$\\

Note: $Q'(x_i) = \prod\limits_{k=1,k\neq i}^{n}(X-x_k)$\\

Cost: Q(X): $O(n^2)$ or $O(M(n)\log n)$\\
Q'($x_i$): $O(n^2)$ or $O(M(n)\log n)$\\
One $L_i$ 1 division n by 1: O(n)\\
Overall $O(n^2)$\\

\underline{Newton (a.k.a divided differences) method}\\
Let P be the interpolation polynomial which is sought. Write P(X) = $(X - x_n)Q(X) + c$ the Euclidean division of P by $X - x_n$, then c = $P(x_n)$. Thus, as $P(x_i) = y_i$, we have $Q(x_i)(x_i-x_n) = P(x_i) - P(x_n) = y_i - y_n$\\

So $Q(x_i) = \frac{y_i - y_n}{x_i - x_n}, i < n$. Q is the solution of an interpolation problem of size n-1.\\

Algorithm: Input: ($x_i,y_i)_{1\leq i \leq n}$\\
If n==1 return $y_1$\\
For i from 1 to n-1 do\\
\indent$z_i \leftarrow \frac{y_i-y_n}{x_i-x_n}$\\
Q $\leftarrow$ Interpolate($x_i,z_i)_{i \leq n-1}$
Return (X-$x_n$)Q + $y_n$\\

Cost: I(n) = I(n-1) + O(n) $\implies$ I(n) = O(n$^2$)

\underline{Fast interpolation}
Lagrange:\begin{align*}
P(X) & = \sum\limits_{i = 1}^n \frac{Q(X)}{(X-x_i)} \frac{y_i}{Q'(x_i)}\\
& = \sum\limits_{i = 1}^{\lfloor n/2 \rfloor}\frac{Q(X)}{(X-x_i)} c_i + \sum\limits_{i = \lfloor n/2 \rfloor+1}^{n}\frac{Q(X)}{(X-x_i)} c_i\\
& = Q_1(X)\sum\limits_{i = 1}^{\lfloor n/2 \rfloor}\frac{Q_0(X)}{(X-x_i)} c_i + Q_0(X)\sum\limits_{i = \lfloor n/2 \rfloor+1}^{n}\frac{Q_1(X)}{(X-x_i)} c_i
\end{align*}

Algorithm:
Input: $(x_i,c_i)$ + product tree for Q(X)\\
If i == 1, return $c_i$\\
$R_0(X)$ = Recursive call $(x_i,c_i)_{i \leq \lfloor n/2 \rfloor}$ + product tree for $Q_0(X)$\\
$R_1(X)$ = Recursive call $(x_i,c_i)_{i > \lfloor n/2 \rfloor}$ + product tree for $Q_1(X)$\\
Return $Q_1(X)R_0(X)+Q_0(X)R_1(X)$\\

Cost: \\
$\begin{array}{c c}
	$Product tree for $Q & O(M(n)log n)\\
	Q' & O(n)\\
	(Q'(x_i))_{1 \leq i \leq n-1} & O(M(n)log n)\\
	(\frac{y_i}{Q'(x_i)})_{1 \leq i \leq n-1} & O(n)\\
\end{array}$\\
+ algorithm on the left:

C(n) = 2C(n/2) + 2M(n/2) + O(n), so C(n) = O(M(n)log n) and overall we have O(M(n)log n) cost.\\

\Rem{}{}{Can be reduced to O(M(n)) if $x_i = \alpha^i, \alpha\in\K$\\}

CRT (Chinese Remainder theorem)
Input: a,m,b,n, m,n coprime\\
Output: x s.t, $x \equiv a \mod m, x \equiv b \mod n$
Return amu+bnv where mu+nv = 1
Cost: same as EEA.

\section{Linear Algebra}
\begin{itemize}
	\item Row echelon form / Gauss pivoting + applications
	\item Characteristic polynomial
\end{itemize}
Mostly will work over $\K$ commutative field\\
Assume that operations over $\K$ have O(1) cost.

Basics:\begin{itemize}
	\item Cost Matrix-vector product = O(mn) O($n^2$) if m = n
	\item Matrix-Matrix = O(mnk) O($n^3$) if m = n = k, O($n^{\log_2 7}$) (Strassen), O($n^{2.37\dots}$) Coppersmith - Winsgrad et al
\end{itemize}

\subsection{Row echelon form}
\Def{}{}{A matrix A $\in M_{m,n}(\K)$ is in row echelon form if
	\begin{enumerate}
		\item its zero rowws are at the bottom of the matrix
		\item The first nonzero coefficient of each row is the right (stricly) of the first non zero coefficient of the previous row
		\item(optional) this 1st nonzero coefficient is 1.
	\end{enumerate}}
	
	\Def{}{}{The matrix is further said to be in reduced row echelon form if further:
		
		4. The first nonzero element in a row the only nonzero element in its column}
	
	\Ex{}{}{$\begin{pmatrix}
		0&1&3&-2&4\\
		0&0&1&0&1\\
		0&0&0&0&1\\
		0&0&0&0&0
		\end{pmatrix}$ r.e.f\\
		
		$\begin{pmatrix}
		0&1&0&-0&0\\
		0&0&1&0&0\\
		0&0&0&0&1\\
		0&0&0&0&0
		\end{pmatrix}$ r.r.e.f}
	
	\Rem{}{}{If M is square, M r.e.f $\implies$ M upper triangular (because of 2.)}
	
	\Prop{}{}{A is r.e.f $\implies \exists$ P a permutation matrix s.t AP $= \begin{pmatrix}
		T&B\\0&0
		\end{pmatrix}$, T upper triangular + invertible\\
		
		A is r.r.e.f $\implies \exists$ P a permutation matrix s.t. AP $= \begin{pmatrix}
		I_n&B\\0&0
		\end{pmatrix}$}
	
	\Proof{Multiplying on the right by a permutation matrix amounts to permuting the columns. It then suffices to note that we obtained the desired forms by moving to column i the column containing the unique nonzero element in row i}
	
	\subsection{Gauss algorithm}
	\subsubsection{Elementary operations}
	Dilations Multiplying on the left by $D_{i\lambda} = Diag(1,1,1,\dots,1,\lambda,1,\dots,1)$ ($\lambda$ at line i)\\
	
	Transitions: Multiply on the left by $I_n + \lambda E_{i,j}, i\neq j$
	
	Swaps: Multiply on left  by (swap matrix, you know)
	
	\subsubsection{Gauss pivoting}
	0.$j \leftarrow 1, l \leftarrow 1\\
	1.$Find a pivot : a non zero element in column j in a row $r\geq l$, If there is non, $j \leftarrow j+1$, goto 1 \\
	2.swap rows r by l\\
	3.(optional) Divide row l by $a_{l,j}$\\
	4.For j from l+1 to m do:
	\indent $L_k \leftarrow L_k - \frac{a_{kj}}{a_{lj}}L_l\\
	j \leftarrow j+1, l \leftarrow l+1$, goto 1\\
	
	Invariants: Before step 1, the part of the matrix which is to the left of column j is r.e.f. + we have a nonzero element in rows $<l$\\
	
	What about r.r.e.f ?
	Change the loop at step 4 in For k from 1 to m, $k \neq j$\\
	
	Cost of this algorithm ?
	Step 1: O(m)\\
	Step 2: O(1) or O(n)\\
	Step 3: O(n)\\
	Step 4: O(mn)\\
	
	Every time we go through step 1, j increases by 1 $\implies$ at most n executions of step 1
	Every time we go through step 2-4, j and l increase by 1 $\implies$ at most min(m,n) executions of steps 2-4.\\
	
	Overall O(mn.min(m,n)) (O($n^3$) if m = n)
	
	\Rem{}{}{The algorithm actually produces a invertible matrix R (which can be computed on the way) s.t RM is in (r.)r.e.f. (e.g RMP $= \begin{pmatrix}
		I_n&B\\0&0
		\end{pmatrix}$ for some permutation matrix P)}
	\subsection{Rank}
	\begin{claim}
		Rank(M) is the number of nonzero rows once M has been put in (r.)r.e.f
	\end{claim}
	
	\begin{proof}
		Let R.M be the r.e.f of M then rk(RM) = rk(M) since R is invertible.\\
		Further, $\exists$ P permutation matrix with RMP = $\begin{pmatrix}
		T&B\\0&0
		\end{pmatrix}$, T upper triangular with 1 on the diagonal., T on size rk(RMP)\\
		So rk(RMP) = rk(M) = \#of non zero rows of RM.
	\end{proof}
	
	\subsection{Determinant (so n = m)}
	\begin{proposition}
		If j $>$ l at some point, then det M = 0
	\end{proposition}
	
	\begin{proof}
		Whenever l increases, j increases simultaneously. So j - l is non decreasing. But if j - l $>$ 0 at the end, rows of index $\geq$ l are 0.
		Otherwise the row echelon form of M is upper triangular with non zero diagonal\\
		
		Case 1: We used step 3; then the r.e.f of M has 1 on the diagonal, and det M = $\frac{-1^{\text{\#swaps}}}{\prod(\text{coeffs at step 3})}$\\
		
		Case 2: We did not use step 3; then det M = $\#\text{swaps}\prod(\text{diagonal terms of r.e.f of M})$
	\end{proof}
	
	\begin{proof}
		In full generality, det R det M  = det (RM) = det(r.e.f of M) = $\prod$(diagonal terms of r.e.f of M)
		
		Without step 3, R is a product of swaps + transvections; swaps have det = -1, so det R $= (-1)^{\#\text{swaps}}$, transvections have det = 1.
		With step 3, R also includes dilatations, and det R  $= (-1)^{\#\text{swaps}} \prod(\text{coeffs used at step 3})$\\
		So if all "principal minors" are $\neq$ 0 we do not have to do swaps.
	\end{proof}
	
	\Rem{}{}{The first time a zero appears on the diagonal during Gauss algorithm, say at line k, is characterized by det(($m_{ij})_{1\leq i,j \leq k}$) = 0}
	
	\subsection{Linear system solving, inverse}
	Assume we want to solve Mx = b.\\
	We can find R,P s.t RMP = $\begin{pmatrix}
	I_k&B\\0&0
	\end{pmatrix}$\\
	Put y = $P^{-1}x$, then MPy = b, so that RMPy = Rb\\
	\begin{itemize}
		\item No solution if Rb does not end with n-k zeroes
		\item Otherwise, y[1,k] = v[1,k] - By[k+1,n] so for each choice of y[k+1,n] we have a solution y[1,n]\\
		
		Notice that $\exists \sigma \in S_n / x_{\sigma(k)} = y_k$ so for each choice of $x_{\sigma(k+1)},\dots,x_{\sigma(n)}$ there is a unique solution x[1,n] 
	\end{itemize}
	
	\begin{remark}
		If we do r.r.e.f on (M b), we'll get (RM Rb) at least if n$\geq$m\\
		In particular, for n = m, doing r.r.e.f. on (M $I_r$) gives ($I_r$, R), with R = $M^{-1}$ in time O$(n^3)$
	\end{remark}
	
	\subsection{Kernel}
	Let M a matrix, and assume RMP = $\begin{pmatrix}
	I_k&B\\0&0
	\end{pmatrix}$, R invertible
	\begin{align*}
	Ker(RMP) =& \{x | RMPx = 0\}\\
	=& \{x | MPx = 0\}\\
	=&\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix}\cdot\begin{pmatrix}
	x_1\\\dots\\x_{n-k}
	\end{pmatrix}
	\end{align*}
	
	Indeed, dim Ker(RMP) = n-k, dim $I_m\cdot\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix} = n-k$\\
	
	$RMP\cdot\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix} = \begin{pmatrix}
	I_k&B\\0&0
	\end{pmatrix}\cdot\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix} = 0\\
	\implies Ker RMP \supseteq Im\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix}$
	
	So Ker RMP = Im$\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix}$\\
	Ker MP = \{$\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix}\cdot\begin{pmatrix}
	u_1\\\dots\\u_{n-k}
	\end{pmatrix}, u_1,\dots,u_{n-k}\in\K$\}\\
	Ker M = P$\cdot\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix} = Im P\cdot\begin{pmatrix}
	-B\\I_{n-k}
	\end{pmatrix}$
	
	\subsection{Image}
	\begin{claim}
		Assume RMP = $\begin{pmatrix}
		I_r&A\\0&0
		\end{pmatrix}$, then Im M has bases R$^{-1}\cdot\begin{pmatrix}
		I_r\\0_{n-r)}
		\end{pmatrix}$
	\end{claim}
	
	\begin{proof}
		Im(RMP) has bases $\begin{pmatrix}
		I_r\\O_{n-r}
		\end{pmatrix}$, P invertible.\\
		Im(RM) = R.Im(M)\\
		so Im(M) has basis R$^{-1}\cdot\begin{pmatrix}
		I_r\\0_{n-r)}
		\end{pmatrix}$
	\end{proof}
	
	\Rem{Final}{}{In the case where we do not do swaps + M square r.e.f can be seen as a decomposition of the form $M = LU$ with L lower tri and U upper tri.\\
		$LUx = b \Leftrightarrow \left\{\begin{array}{c c c}
		Ux & = y & O(n^2)\\
		Ly & = b & O(n^2)
		\end{array}
		\right.$ Useful if one wants to solve many linear systems with M}
	
	\subsection{Characteristic polynomial}
	
	\textbf{\underline{Warning:} Gauss pivoting has \underline{nothing} to do with eigenvalues}
	
	To preerve eignevalues, one must use operations $M \rightarrow N^{-1}MN$ whereas Gauss uses $M \rightarrow NM$
	\subsubsection{Hessenberg form}
	(see TD)
	Adaptation of Gauss using transformations $M \rightarrow N^{-1}MN$
	\subsubsection{Le Verrier's method}
	\begin{lemma}
		Newton's identities\\
		Let P = $\sum_{i = 0}^{n}p_{n-i} X^i \in \K[X]$, Let $\alpha_1,\dots,\alpha_n$ be its roots with multiplicities.\\
		Define the Newton sums $S_k(P) = \sum_{i = 0}^{n} \alpha_i^k$\\
		Then, $\forall t < n,$ we have $\sum_{i = 0}^{t} p_iS_{t-i}(P) = (n-t)p_t$
	\end{lemma}
	We can deduce $p_1,p_2,\dots,p_n$ from $S_1,\dots,S_n$ and the choice $p_0 = 1$ in time O($n^2$)\\
	
	Say now M is a matrix, P its characteristic polynomial. $S_1(P) = Tr(M), S_k(P) = Tr(M^k)$\\
	
	Algorithm:
	Compute Tr(M),Tr(M$^2$),\dots,Tr(M$^n$) and use Newton identities to recover P.
	
	Complexity: O($n^4)$
\subsubsection{Multimodular methods}
Works for lots of situations :\\
- Determinant :\\
Say we want det(M), $M \in M_n(\Z), M_n(\K[X])$\\
Idea :\begin{enumerate}
	\item Get a bound on det M,\\
	- $M \in M_n(\Z)$\\
	$|\det M| \leq n!||M||_\infty^n =: \delta$ or $\prod_{i=1}^{n} ||m_i||_2$
	- $M \in \K[X]$\\
	deg det M $\leq$ n maxdeg($n_ij) =: \delta$
	\item ($\K[X]$) pick $\delta + 1$ points $u_0,\dots,u_\delta$ compute det M($u_i)$. Interpolate to get det(M).
	\item[2'.] ($\Z$) pick $p_1,\dots,p_r$ primes s.t $\prod p_i \geq 2\delta'+1$\\
	Compute det M mod $p_i$.\\
	Do CRT to deduce det M.
\end{enumerate}
Cost of (2).
$n^2$ evaluations at $\delta +1$ points of poly of degree $\leq \delta/n$. O($n\delta^2$) naively. O($n^2M(\delta)\log\delta$) with multple point evaluation.\\
det M($u_i) \hookrightarrow O(n^3(\delta + 1))$\\
Interpolation $\hookrightarrow O(\delta^2)$ (naive) or $O(M(\delta)\log\delta)$ (fast)\\\\
Overall, naive: $O(n^3\delta + n\delta^2)$\\
\indent fast: $O(n^3\delta + n^2M(\delta)\log\delta)$\\

We trade arithmetic on objects of size $\delta$ for $\delta$ times computation on scalar objects of size O(1).

\subsubsection{Structured matrices}
Basic idea: some families of matrices have an interpretation in terms of polynomials $\hookrightarrow$ gives faster algorithms\\

Circulante matrix:\\
A matrix A = $\begin{pmatrix}
a_1&a_n&\dots&a_2\\
a_2&a_3&\dots&a_3\\
&\dots\\
a_n&a_{n-1}&\dots&a_1
\end{pmatrix}$
(that is, the (i,j) coeff of A depends only on (i-j) mod n.)

\Lem{}{}{The set of dim n circulant matrices is a ring isomorphic to $\K[X]/(X^n - 1)$}
\begin{proof}
	Let P = $\begin{pmatrix}
	0&0&\dots&1\\
	1&0&\dots&0\\
	&\dots\\
	0&\dots&1&0
	\end{pmatrix}$ = C(0,1,0,\dots,0)\\
	Then C($a_1,\dots,a_n) = \sum_{i = 0}^{n-1} a_{i+1} P^i$\\
	So $\phi:\K[X] \rightarrow$ circulant matrix, $Q \mapsto Q(P)$ is a ring homeomorphism.\\
	$\phi$ is surjective.\\
	Ker $\phi$ is generated by the minimal polynomial of P which is $X^n -1 $\\
	Overall $\K[X]/Ker \phi \approx Im \phi$
\end{proof}
\Cor{}{}{The product of two circulant matrices can be computed in O(M(n))}
\begin{proof}
	Compute $\phi(\phi^{-1}(A).\phi^{-1}(B))$, $\phi$ is linear, a product on (n-1) degree poly, and a division by $X^n - 1$ ($\equiv$ one degree n addition)\\
	The same is true for C($a_1,\dots,a_n) \times v$ (compute $C(a_1,\dots,a_n)\times C(v_1,\dots,v_n)$ keep the first column.)
\end{proof}
Define $\Omega = (\omega_n^{ij}), \omega_n$ is a primitive n-th root of 1. Then $\Omega^{-1} C(a_1,\dots,a_n)\Omega$ is diagonal.
\begin{proof}
	\begin{align*}
	(C(a_1,\dots,a_n)\Omega)_{ij} = &\sum_{k = 1}^{n}c_{ik}\omega^{kj}\\
	= &\sum_{k = 1}^{n}a_{(i-k mod n)}\omega^{kj}\\
	= & \omega^{ij} \sum_{k=1}^n a_{(i-k mod n)}\omega^{j(i-k)}\\
	= & \omega^{ij} \sum_{k=1}^n a_{k}\omega^{jk}\\
	= & \Omega . diag(\sum_{k=1}^{n}a_k\omega^{jk})
	\end{align*}
\end{proof}
\begin{corollary}
	The determinant/inverse/linear sysyem solving can be computed in O(M(n))
\end{corollary}
\begin{proof}
	Use FFT + (inverse, system solving) inverse FFT.
	
Det: det C($a_1,\dots,a_n) = \det(\Omega^{-1} C(a_1,\dots,a_n) \Omega) = \det(diag(DFT(\sum_{k=1}^{n}a_kX^k)))$\\
DFT$_{n,\omega}$ costs O(M(n)) + n multiplies $\implies$ O(M(n))

Inverse: $\Omega^{-1} C(a_1,\dots,a_n) \Omega = diag(DFT(\sum_{k=1}^{n}a_kX^k))$\\
So $\Omega^{-1} C^{-1}(a_1,\dots,a_n) \Omega = diag(\frac{1}{DFT(\sum_{k=1}^{n}a_kX^k)})$\\
So $C^{-1}(a_1,\dots,a_n) = C(b_1,\dots,b_n)$ with $(b_1,\dots,b_n) = DFT_{n,\omega}(\frac{1}{DFT_{n,\omega}(\sum_{k=1}^{n}a_kX^k)})$
\end{proof}

\Rem{}{}{We've only studied FFT in the "nicest" case n $= 2^k$, the algo here require to do a real, general FFT of order n, not so easy.}

\subsubsection{Vandermonde matrices}
V($X_1,\dots,X_n$) = ($X_i^{j-1})$\\
Cost of Vandermonde x vector product ?

$V(X_1,\dots,X_n) = \begin{pmatrix}
p_0\\\dots\\p_{n-1}
\end{pmatrix} = \begin{pmatrix}
P(X_1)\\\dots\\P(X_n)
\end{pmatrix}$ where P(X) = $\sum_{i = 0}^{n-1} p_iX^i$

So O(M(n)log n) using multiple point interpolation. O(M(n)log n) for linear system solving using interpolation\\

Exercice: What can you do for the inverse ?
\subsubsection{Cauchy}
$C(X_i,Y_j) = (\frac{1}{X_i-Y_j})\\
C(X_i,Y_j)\times\begin{pmatrix}
v_1\\\dots\\v_n
\end{pmatrix} = \begin{pmatrix}
R(X_1)\\\dots\\R(X_n)
\end{pmatrix}$ where R(X) = $\sum_{k=1}^n \frac{v_k}{X - Y_k}$

Compute R(X) efficiently ?
R(X) = P(X)/Q(X)\\
We do a product tree. R = $P_1/Q_1 + P_2/Q_2$ and $Q=Q_1Q_2, P=P_1Q_2+P_2Q_1$

Cost: C(n) = 2C(n/2) + 3M(n/2) + O(n)\\
Overall, C(n)  = O(M(n)log n)\\

Now, compute (P($X_i$)), ($Q(X_i)$), return ($\frac{P(X_i)}{Q(X_i)})$)\\

Exercise: Do linear system solving

\subsection{Fast linear algebra}
\subsubsection{Strassen algorithm}
\begin{theorem}
	One can compute the product of 2 nxn matrices in times O(n$^{\log_2 7}$)
\end{theorem}
\begin{proof}
	Find ugly formulas to compute product of 2x2 matrices using 7 multiplies If A,B have size 2n, use the formulas to computes AxB as the product of two 2x2 matrices with nxn block coefficients.\\
	Using the forlmulas, reduce 2nx2n mults to 7 nxn mults $\implies$ MM(2n) = 7 MM(n) + O(n$^2$) which give MM(n) = $O(n^{\log_2 7})$
\end{proof}

\begin{remark}
	\begin{enumerate}
		\item Multiplying $\begin{pmatrix}
		a&b\\c&d
		\end{pmatrix} = A$ by  $\begin{pmatrix}
		t&u\\v&w
		\end{pmatrix}$ is the same as computing  $\begin{pmatrix}
		A&0\\0&A
		\end{pmatrix} $ $\begin{pmatrix}
			t\\v\\u\\w
		\end{pmatrix}$
		
		 $\begin{pmatrix}
		 A&0\\0&A
		 \end{pmatrix} = aE_1 + dE_2 + E_3 + (*E_4:*) (b-a)(E_{12} - E_{32}) + (*E_5:*)(b-d)(E_{32} + E_{34}) + (E_6 + E_7) (\text{transpose with b replaced by c})$
		 
		 $E_1$ 4 ones on the top-left, $E_2$ 4 ones on the bottom-right, $E_3$ is  $\begin{pmatrix}
		 d-a&a-d\\a-d&d-a
		 \end{pmatrix}$ in the center.
		 \item $\forall i, E_i \begin{pmatrix}
		 	t\\v\\u\\w
		 \end{pmatrix}$ can be done in 1 multiplication.
	\end{enumerate}
\end{remark}

\subsubsection{Fast inversion}
We'll make a "genericity" assumption, namely that plenty of submatrice are invertible.
\begin{lemma}
	 $\begin{pmatrix}
	 a&b\\c&d
	 \end{pmatrix} = 
	 \begin{pmatrix}
	 	1&0\\ca^{-1}&1
	 \end{pmatrix}
	  \begin{pmatrix}
	  a&0\\0&sc
	  \end{pmatrix}
	   \begin{pmatrix}
	   	1&a^{-1}b\\0&1
	   \end{pmatrix}$ where sc = d - c$a^{-1}b$
\end{lemma}
This is just one version of Gauss pivoting on a 2x2 matrix + avoid using commutativity so that the identity still holds if a,b,c,d are matrices.

\begin{corollary}
	$\begin{pmatrix}
	a&b\\c&d
	\end{pmatrix} = 
	\begin{pmatrix}
	1&-a^{-1}b\\0&1
	\end{pmatrix}
	\begin{pmatrix}
	a&0\\0&sc^{-1}
	\end{pmatrix}
	\begin{pmatrix}
	1&0\\-ca^{-1}&1
	\end{pmatrix}$ where sc = d - c$a^{-1}b$
\end{corollary}
Algo: Inverse(A)\\
Recursive call of size n/2 to get $a^{-1}$ compute sc + recursive call to get $sc^{-1}$, compute the product above.
Cost: MI(n) = 2MI(n/2) + O(MM(n/2)) -> MI(n) = O(MM(n)) (recall that MM(n)) >> $n^2$
\end{document}